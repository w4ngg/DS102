{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition\n",
    "\n",
    "Given a dataset $D = (X, C)$ where $C = \\{C_1, C_2, ..., C_k\\}$ is the set of all classes, the classification task requires us to classify correctly any input $x_i \\in \\mathbb{R}^K$ to its respective class $c \\in C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classification Task\n",
    "\n",
    "Let consider a classification task where $k = 2$ (a binary classification task). Let $P(x, C_i)$ is the probability of assigning class $C_i \\in C$ to the given input $x \\in X$. Then the binary classification task becomes a joint probability approximation task.\n",
    "\n",
    "Applying the Bayesian rule, we have:\n",
    "\n",
    "$$\n",
    "    P(x, C_i) = P(x | C_i)P(C_i) = P(C_i | x)P(x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\Rightarrow P(C_i | x) = \\frac{P(x | C_i)P(C_i)}{P(x)}\n",
    "$$\n",
    "\n",
    "$P(x | C_i)$ is called the likelihood distribution, $P(C_i)$ is the prior distribution, $P(x)$ is the evidence, and $P(C_i | x)$ is the posterior distribution.\n",
    "\n",
    "Following the Bayesian rule, we have:\n",
    "\n",
    "\\begin{align}\n",
    "    P(C_i | x)  & = \\frac{P(x | C_i)P(C_i)}{P(x)} \\\\\n",
    "                & = \\frac{P(x | C_i) P(C_i)}{ P(x, C_1) + P(x, C_2) } \\\\\n",
    "                & = \\frac{P(x | C_i) P(C_i)}{ P(x | C_1)P(C_1) + P(x | C_2)P(C_2) } \\\\\n",
    "\\end{align}\n",
    "\n",
    "Let $a_1 = ln\\frac{P(x | C_1)P(C_1)}{P(x | C_2)P(C_2)}$, we have:\n",
    "\n",
    "\\begin{align}\n",
    "    P(C_1 | x)  & = \\left( \\frac{P(x | C_1)P(C_1) + P(x | C_2)P(C_2)}{P(x | C_1)P(C_1)} \\right)^{-1} \\\\\n",
    "                & = \\left(1 + \\frac{P(x | C_2)P(C_2)}{P(x | C_1)P(C_1)} \\right)^{-1} \\\\\n",
    "                & = \\left(1 + \\left(\\frac{P(x | C_1)P(C_1)}{P(x | C_2)P(C_2)}\\right)^{-1} \\right)^{-1} \\\\\n",
    "                & = \\left(1 + exp \\left(-ln\\frac{P(x | C_1)P(C_1)}{P(x | C_2)P(C_2)}\\right) \\right)^{-1} \\\\\n",
    "                & = (1 + exp(-a_1))^{-1} \\\\\n",
    "                & = \\frac{1}{1 + exp(-a_1)} \\\\\n",
    "                & = \\sigma(a_1)\n",
    "\\end{align}\n",
    "\n",
    "The function $\\sigma: \\mathbb{R} \\mapsto [0, 1], x \\mapsto \\sigma(x)$ is the **Sigmoid function** and has the graph as the following figure.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "By the same way, via defining $a_2 = ln\\frac{P(x | C_2)P(C_2)}{P(x | C_1)P(C_1)}$, we also have:\n",
    "\n",
    "$$\n",
    "    P(C_2 | x) = \\sigma(a_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Task in General\n",
    "\n",
    "Let us consider the general case where $k \\in \\mathbb{N}$ and $k > 2$. In this case, we have:\n",
    "\n",
    "$$\n",
    "    P(C_i | x) = \\frac{P(x | C_i)P(C_i)}{P(x)} = \\frac{P(x | C_i)P(C_i)}{\\sum_{j=1}^{k} P(C_j, x)} = \\frac{P(x | C_i)P(C_i)}{\\sum_{j=1}^{k} P(x | C_j)P(C_j)}\n",
    "$$\n",
    "\n",
    "Let define $a_i = lnP(x | C_i)P(C_i)$, we have:\n",
    "\n",
    "\\begin{align}\n",
    "    P(C_i | x)  & = \\frac{exp(ln(P(x | C_i)P(C_i)))}{\\sum_{j=1}^{k} exp(ln(P(x | C_j)P(C_j)))} \\\\\n",
    "                & = \\frac{exp(a_i)}{\\sum_{j=1}^{k} exp(a_j)}\n",
    "\\end{align}\n",
    "\n",
    "The left handside of (2) is called the **Softmax function** on the vector $a = (a_1, a_2, ..., a_k) \\in \\mathbb{R}^k$.\n",
    "\n",
    "From former analysis, we can conclude:\n",
    "- In case $k = 2$, the posterior distribution has the form of sigmoid function.\n",
    "- In case $k > 2$, the posterior distribution has the form of softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "In case of binary classification, the posterior has the form of\n",
    "\n",
    "$$\n",
    "    P(C_i | x) = \\frac{P(x | C_1)P(C_1)}{P(x | C_1)P(C_1) + P(x | C_2)P(C_2)}\n",
    "$$\n",
    "\n",
    "Let $a_1 = ln\\frac{P(x | C_1)P(C_1)}{P(x | C_2)P(C_2)}$, then\n",
    "\n",
    "\\begin{align}\n",
    "    a_1 & = ln (P(x | C_1)P(C_1)) - ln (P(x | C_2)P(C_2)) \\\\\n",
    "        & = ln P(x | C_1) - ln P(x | C_2) + const \\\\\n",
    "\\end{align}\n",
    "where $const \\in \\mathbb{R}$ indicates terms that are not relevant to $x$.\n",
    "\n",
    "Let assume that $P(x | C_1) \\sim \\mathcal{N}(\\mu_1, \\Sigma_1)$ and $P(x | C_2) \\sim \\mathcal{N}(\\mu_2, \\Sigma_2)$ where $\\mu_1, \\mu_2 \\in \\mathbb{R}^k$ and $\\Sigma_1, \\Sigma_2 \\in \\mathbb{R}^{k \\times k}$.\n",
    "\n",
    "From that on, we have:\n",
    "\n",
    "$$\n",
    "    P(x | C_1) = \\frac{1}{(2\\pi)^{\\frac{k}{2}}} \\frac{1}{|\\Sigma_1|^{\\frac{1}{2}}} exp(-\\frac{1}{2}(x - \\mu_1)^T\\Sigma_1(x - \\mu_1))\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\Rightarrow lnP(x | C_1) = -\\frac{k}{2}ln(2\\pi) - \\frac{1}{2}ln|\\Sigma_1| - \\frac{1}{2}(x - \\mu_1)^T\\Sigma_1(x - \\mu_1)\n",
    "$$\n",
    "\n",
    "In the same way, we have\n",
    "\n",
    "$$\n",
    "    \\Rightarrow lnP(x | C_2) = -\\frac{k}{2}ln(2\\pi) - \\frac{1}{2}ln|\\Sigma_2| - \\frac{1}{2}(x - \\mu_2)^T\\Sigma_2(x - \\mu_2)\n",
    "$$\n",
    "\n",
    "Replace these terms into $a_1$, we have:\n",
    "\n",
    "\\begin{align}\n",
    "    a_1 & = lnP(x | C_1)P(C_1) - lnP(x | C_2)P(C_2) + const \\\\\n",
    "        & = \\left[-\\frac{k}{2}ln(2\\pi) - \\frac{1}{2}ln|\\Sigma_1| - \\frac{1}{2}(x - \\mu_1)^T\\Sigma_1(x - \\mu_1)\\right] - \\left[-\\frac{k}{2}ln(2\\pi) - \\frac{1}{2}ln|\\Sigma_2| - \\frac{1}{2}(x - \\mu_2)^T\\Sigma_2(x - \\mu_2)\\right] + const \\\\\n",
    "        & = -\\frac{1}{2}(ln|\\Sigma_1| - ln|\\Sigma_2|) - \\frac{1}{2}(x^T\\Sigma_1^{-1}x - x^T\\Sigma_2^{-1}x) - \\frac{1}{2}(\\mu_1^T\\Sigma_1^{-1}\\mu_1 - \\mu_2^T\\Sigma_2^{-1}\\mu_2) + (\\mu_1^T\\Sigma_1^{-1}x - \\mu_2^T\\Sigma_2^{-1}x) + const \\\\\n",
    "\\end{align}\n",
    "\n",
    "Let assume that $\\Sigma_1 = \\Sigma_2 = \\Sigma$, then we have:\n",
    "\n",
    "\\begin{align}\n",
    "    a_1 & = lnP(x | C_1)P(C_1) - lnP(x | C_2)P(C_2) + const \\\\\n",
    "        & = -\\frac{1}{2}(ln|\\Sigma_1| - ln|\\Sigma_2|) - \\frac{1}{2}(x^T\\Sigma_1^{-1}x - x^T\\Sigma_2^{-1}x) - \\frac{1}{2}(\\mu_1^T\\Sigma_1^{-1}\\mu_1 - \\mu_2^T\\Sigma_2^{-1}\\mu_2) + (\\mu_1^T\\Sigma_1^{-1}x - \\mu_2^T\\Sigma_2^{-1}x) + const \\\\\n",
    "        & = -\\frac{1}{2}(ln|\\Sigma| - ln|\\Sigma|) - \\frac{1}{2}(x^T\\Sigma^{-1}x - x^T\\Sigma^{-1}x) - \\frac{1}{2}(\\mu_1^T\\Sigma^{-1}\\mu_1 - \\mu_2^T\\Sigma^{-1}\\mu_2) + (\\mu_1^T\\Sigma^{-1}x - \\mu_2^T\\Sigma^{-1}x) + const \\\\\n",
    "        & = - \\frac{1}{2}(\\mu_1^T\\Sigma^{-1}\\mu_1 - \\mu_2^T\\Sigma^{-1}\\mu_2) + (\\mu_1^T - \\mu_2^T)\\Sigma^{-1}x + const \\\\\n",
    "\\end{align}\n",
    "\n",
    "By defining\n",
    "\n",
    "$$\n",
    "    w_0 = - \\frac{1}{2}(\\mu_1^T\\Sigma^{-1}\\mu_1 - \\mu_2^T\\Sigma^{-1}\\mu_2) + const \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    w_1 = (\\mu_1^T - \\mu_2^T)\\Sigma^{-1} \\in \\mathbb{R}^k\n",
    "$$\n",
    "\n",
    "then we have\n",
    "\n",
    "$$\n",
    "    a_1 = w_0 + w_1 x\n",
    "$$\n",
    "\n",
    "In addition, we have shown that \n",
    "\n",
    "$$\n",
    "    P(C_1 | x) = \\sigma(a_1) = \\sigma(w_0 + w_1x)\n",
    "$$\n",
    "\n",
    "then the above formula is called the **Logistic Regression** model.\n",
    "\n",
    "In conclusion, with the assumption of Gaussian distribution of the likelihood probability $P(x | C_i)$ of the class $C_i$, then we have the Logistic Regression for the posterior distribution of that class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Regression\n",
    "\n",
    "Consider the general case where $k > 2$, we have:\n",
    "\n",
    "$$\n",
    "    P(C_i | x) = \\frac{P(x | C_i)P(C_i)}{P(x)} = \\frac{P(x | C_i)P(C_i)}{\\sum_{j=1}^{k} P(C_j, x)} = \\frac{P(x | C_i)P(C_i)}{\\sum_{j=1}^{k} P(x | C_j)P(C_j)}\n",
    "$$\n",
    "\n",
    "Let define $a_i = lnP(x | C_i)P(C_i)$, we have:\n",
    "\n",
    "\\begin{align}\n",
    "    P(C_i | x)  & = \\frac{exp(ln(P(x | C_i)P(C_i)))}{\\sum_{j=1}^{k} exp(ln(P(x | C_j)P(C_j)))} \\\\\n",
    "                & = \\frac{exp(a_i)}{\\sum_{j=1}^{k} exp(a_j)}\n",
    "\\end{align}\n",
    "\n",
    "As the same approach, let assume the likelihood distribution follows the Gaussian distribution $P(x | C_i) \\sim \\mathbb{N}(\\mu_i, \\Sigma_i)$. Then we have:\n",
    "\n",
    "$$\n",
    "    P(x | C_i) = \\frac{1}{(2\\pi)^{\\frac{k}{2}}} \\frac{1}{|\\Sigma_i|^{\\frac{1}{2}}} exp(-\\frac{1}{2}(x - \\mu_i)^T\\Sigma_i(x - \\mu_i))\n",
    "$$\n",
    "\n",
    "\\begin{align}\n",
    "    \\Rightarrow lnP(x | C_i)    & = -\\frac{k}{2}ln(2\\pi) - \\frac{1}{2}ln|\\Sigma_i| - \\frac{1}{2}(x - \\mu_i)^T\\Sigma_i(x - \\mu_i) \\\\\n",
    "                                & = -\\frac{k}{2}ln(2\\pi) - \\frac{1}{2}ln|\\Sigma_i| - \\frac{1}{2}x^T\\Sigma_i^{-1}x - \\frac{1}{2}\\mu_i^T\\Sigma_i^{-1}\\mu_i + \\mu_i^T\\Sigma_i^{-1}x\n",
    "\\end{align}\n",
    "\n",
    "Hence\n",
    "\n",
    "\\begin{align}\n",
    "    exp(a_i)    & = exp\\left( -\\frac{k}{2}ln(2\\pi) - \\frac{1}{2}ln|\\Sigma_i| - \\frac{1}{2}x^T\\Sigma_i^{-1}x - \\frac{1}{2}\\mu_i^T\\Sigma_i^{-1}\\mu_i + \\mu_i^T\\Sigma_i^{-1}x + lnC_i \\right) \\\\\n",
    "                & = exp\\left( -\\frac{k}{2}ln(2\\pi) - \\frac{1}{2}ln|\\Sigma_i| - \\frac{1}{2}x^T\\Sigma_i^{-1}x\\right)exp\\left( - \\frac{1}{2}\\mu_i^T\\Sigma_i^{-1}\\mu_i + \\mu_i^T\\Sigma_i^{-1}x + lnC_i \\right)\n",
    "\\end{align}\n",
    "\n",
    "Now assume that $\\Sigma_1 = \\Sigma_2 = \\Sigma_3 = ... = \\Sigma_k = \\Sigma$, we have:\n",
    "\n",
    "\\begin{align}\n",
    "        P(C_i | x)  & = \\frac{exp(a_i)}{\\sum_{j=1}^{k} exp(a_j)} \\\\\n",
    "                    & = \\frac{exp\\left( -\\frac{k}{2}ln(2\\pi) - \\frac{1}{2}ln|\\Sigma| - \\frac{1}{2}x^T\\Sigma^{-1}x\\right)exp\\left( - \\frac{1}{2}\\mu_i^T\\Sigma^{-1}\\mu_i + \\mu_i^T\\Sigma^{-1}x + lnC_i \\right)}{\\sum_{j=1}^k exp\\left( -\\frac{k}{2}ln(2\\pi) - \\frac{1}{2}ln|\\Sigma| - \\frac{1}{2}x^T\\Sigma^{-1}x\\right)exp\\left( - \\frac{1}{2}\\mu_j^T\\Sigma^{-1}\\mu_j + \\mu_j^T\\Sigma^{-1}x + lnC_j \\right)} \\\\\n",
    "                    & = \\frac{exp\\left( - \\frac{1}{2}\\mu_i^T\\Sigma^{-1}\\mu_i + \\mu_i^T\\Sigma^{-1}x + lnC_i \\right)}{\\sum_{j=1}^k exp\\left( - \\frac{1}{2}\\mu_j^T\\Sigma^{-1}\\mu_j + \\mu_j^T\\Sigma^{-1}x + lnC_j \\right)} \\\\\n",
    "                    & = \\frac{exp\\left( - \\frac{1}{2}\\mu_i^T\\Sigma^{-1}\\mu_i + lnC_i + \\mu_i^T\\Sigma^{-1}x \\right)}{\\sum_{j=1}^k exp\\left( - \\frac{1}{2}\\mu_j^T\\Sigma^{-1}\\mu_j + lnC_j + \\mu_j^T\\Sigma^{-1}x \\right)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Let define \n",
    "\n",
    "$$\n",
    "    w_{i0} = -\\frac{1}{2}\\mu_i^T\\Sigma^{-1}\\mu_i + lnC_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "    w_{i1} = \\mu_i^T\\Sigma^{-1}x\n",
    "$$\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "    P(C_i | x) = \\frac{exp(w_{i0} + w_{i1}x)}{\\sum_{j=1}^{k} exp(w_{j0} + w_{j1}x)}\n",
    "$$\n",
    "\n",
    "The above formula is called the **Softmax Regression** model.\n",
    "\n",
    "In conclusion, with the assumption of Gaussian distribution of the likelihood probability $P(x | C_i)$ of the class $C_i$, then we have the Softmax Regression for the posterior distribution of that class."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
